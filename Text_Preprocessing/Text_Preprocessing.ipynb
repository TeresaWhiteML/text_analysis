{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Text Preprocessing in Natural Language Processing (NLP)**"
      ],
      "metadata": {
        "id": "WEU84Gr8vyRw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to use machine learning models for the analysis of a text data to predict text similarities. To prepare our dataset for this, we take some specific steps to clean the text data in order to get faster and efficient results. We use **Natural Language processing (NLP)** for this. NLP is a branch of Data Science that helps analyzing Text data.\n",
        "\n",
        "I will follow some of the steps suggested in chapter 4 of Anandarajan book (\"Practical Text Analytics\" **[Link](https://https://link.springer.com/chapter/10.1007/978-3-319-95663-3_4)**). \n",
        "\n",
        "*   Each instance in the collected text data should have a unique identifier (known as documents).\n",
        "*   Many of those documents builds a document collection (known as corpus).\n",
        "\n",
        "The steps of the text data pre-processing process by this author are the following:\n",
        "\n",
        "1.   Unitize & Tokenize\n",
        "2.   Standardize & Cleanse\n",
        "3.   Stop Word Removal\n",
        "4.   Stem or Lemmatize\n",
        "\n"
      ],
      "metadata": {
        "id": "-Xf1XWpttMFU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Some Common Text Preprocessing Steps**"
      ],
      "metadata": {
        "id": "v5IXbetsyQVT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The text preprocessing process is an important step taken before starting the text analysis because it will remove all the unnecessary information from the raw text data to make the analysis process smoother. **This process would usually take longer than the analysis itself.**\n"
      ],
      "metadata": {
        "id": "xBkSDWCnyiuN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Unitize and Tokenize:**"
      ],
      "metadata": {
        "id": "ozaYswzON53O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   We choose the unit of text to analyze. It can be a word or a group of words.\n",
        "*   Tokenization is the process of splitting the text into smaller units.\n",
        "*   Depending on the problem that we want to solve, we can choose to use word or sentence tokenization.\n",
        "*   In this case, the grammar and the order of the text are not considered when we look for the quantitative representation of the text data. This is known as the bag-of-words model, in which the representation of the words in a text takes no notice of the grammar and word order keeping the multiplicity of the words."
      ],
      "metadata": {
        "id": "w3iaPWQGOIks"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**N-grams Model:** \n",
        "\n",
        "*  N-grams are tokens that are continuous word sequences with a certain length N. For example, if we have $n=1$, we have a 1-grams (unigram) which is a token composed by one word. If we have $n=2$, we have a 2-grams (bigram) which are tokens composed by two consecutive words, and so on."
      ],
      "metadata": {
        "id": "SAQPazQVg2QK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  An example of a bigram: \n",
        "\n",
        "**Text example:** \"Contract terms evolve in response to their environments, including new laws.\"\n",
        "\n",
        "Here we show the tokens in bold text:\n",
        "\n",
        "*  1st token: **Contract terms** evolve in response to their environments, \n",
        "including new laws.\n",
        "*  2nd token: Contract **terms evolve** in response to their environments, \n",
        "including new laws.\n",
        "*  3rd token: Contract terms **evolve in** response to their environments, \n",
        "including new laws.\n",
        "*  4th token: Contract terms evolve **in response** to their environments, \n",
        "including new laws.\n",
        "*  5th token: Contract terms evolve in **response to** their environments, \n",
        "including new laws.\n",
        "*  6th token: Contract terms evolve in response **to their** environments, \n",
        "including new laws.\n",
        "*  7th token: Contract terms evolve in response to **their environments**, \n",
        "including new laws.\n",
        "*  8th token: Contract terms evolve in response to their **environments,** \n",
        "including new laws.\n",
        "*  9th token: Contract terms evolve in response to their environments**,** **including** new laws.\n",
        "*  10th token: Contract terms evolve in response to their environments,**including new** laws.\n",
        "*  11th token: Contract terms evolve in response to their environments, \n",
        "including **new laws**.\n",
        "*  12th token: Contract terms evolve in response to their environments, \n",
        "including new **laws.**"
      ],
      "metadata": {
        "id": "X0nfi05Alvs5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Standardization and Cleaning:**"
      ],
      "metadata": {
        "id": "ocxRAcQrllPn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this step, we clean the tokens in order to not have problems when we find the multiplicity of the tokens. For example, two tokens have the same word but one is in uppercase and the other one is in lowercase. They are the same word but they would be considered different tokens because they have different letter case. Removing these special characters are also known as normalization."
      ],
      "metadata": {
        "id": "7b4J6Jr1tOJL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some of the steps to conisder are the following:\n",
        "\n",
        "*  Convert the words in the text to lower case\n",
        "*  Remove numbers, punctuation, and any special characters, links, hashtags, among others.\n",
        "*  Remove any extra (white) space"
      ],
      "metadata": {
        "id": "oaZuhwaTtOyi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text example:** \"**C**ontract terms evolve in response to their environments**,** including new laws**.**\"\n",
        "\n",
        "**Clean Text example:** \"contract terms evolve in response to their environments including new laws\""
      ],
      "metadata": {
        "id": "D2Z9OXUFyEvC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.   Stop Word Removal**\n"
      ],
      "metadata": {
        "id": "6poKPtk9tPCz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stopwords are the common words that do not add any valuable information to the analysis such as a, about, an, as, that, among others.\n",
        "\n",
        "A good list of stopwords in English (and other languages) can be found in this **[link](https://www.ranks.nl/stopwords)**."
      ],
      "metadata": {
        "id": "-9Mjv_vUy8SZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text example:** \"contract terms evolve **in** response **to their** environments including **new** laws\"\n",
        "\n",
        "**Clean Text example:** \"contract terms evolve response environments including laws\""
      ],
      "metadata": {
        "id": "E9dkX-sG2DSa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comment:**\n",
        "\n",
        "*  Some projects will have certain terms with high frequency and they might not add any value into the analysis. In this case, we build our own dictionary."
      ],
      "metadata": {
        "id": "xsoWuMEz4HYi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.   Stem or Lemmatize**"
      ],
      "metadata": {
        "id": "aA-yRQh9yvbY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to conisder two concepts:\n",
        "\n",
        "* **Syntax:** It studies sentence structure, grammar and parts of speech. \n",
        "\n",
        "* **Semantics:** It studies the meaning of the sentences. It covers synonymy (same meaning for two different words) and polysemy (single word with multiple meanings)."
      ],
      "metadata": {
        "id": "IYb0NmoCyveY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Stemming**"
      ],
      "metadata": {
        "id": "mQCBeUrDyvhw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* It reduces words to their root word.\n",
        "* This helps to find more unique tokens.\n",
        "* Words with the same root, often, share the same meaning but not always.\n",
        "* Stemming would also depend on the project (willing to increase the errors or not)."
      ],
      "metadata": {
        "id": "DqZWbZJSIeHo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Lemmatization**"
      ],
      "metadata": {
        "id": "comJjQcntPXr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  Stemming has an issue when a word has multiple meanings.\n",
        "*  Lemmatization takes care of this issue by considering the morphological analysis of the words.\n",
        "*  Lemmatization groups tokens considering the part of the speech."
      ],
      "metadata": {
        "id": "JfUmpWxmNfHI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Code**"
      ],
      "metadata": {
        "id": "loMkUY6STfkQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is an example on applying tokenization in Python: (Please, see the Text_Preprocessing_code for more examples)"
      ],
      "metadata": {
        "id": "P5mY9d27qLdp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Zc1UJQwkFyW",
        "outputId": "7db2ae50-232f-465a-c343-6d0ebd4fff79"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenization\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqKwIxHutPiT",
        "outputId": "4d4a0532-7b44-4572-efe5-082014be5ca6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example of a text with 3 sentences\n",
        "text_data = \"This is the first sentence. Then, it comes the second sentence. Finally, this is the last sentence.\"\n",
        "\n",
        "#sent_tokenize divides the text into different lines\n",
        "nltk_tokens1 = sent_tokenize(text_data)\n",
        "print(nltk_tokens1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMM-GKE4klRv",
        "outputId": "e95e5e1d-51e0-4ce3-ef46-c2c003541bf8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This is the first sentence.', 'Then, it comes the second sentence.', 'Finally, this is the last sentence.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#word_tokenize divides the text into words\n",
        "nltk_tokens2 = word_tokenize(text_data)\n",
        "print(nltk_tokens2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3bLMtTvkZgu",
        "outputId": "1d6ac252-4dd2-4d35-97ab-9dfd8527fca0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'the', 'first', 'sentence', '.', 'Then', ',', 'it', 'comes', 'the', 'second', 'sentence', '.', 'Finally', ',', 'this', 'is', 'the', 'last', 'sentence', '.']\n"
          ]
        }
      ]
    }
  ]
}